{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcio/.virtualenvs/AvaliacoesGCC151_env/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n",
      "/home/marcio/.virtualenvs/AvaliacoesGCC151_env/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "from utils.lexical import normalizador\n",
    "normalizer = normalizador.Normalizador()\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting corpora files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_path = '../data/corpora/'\n",
    "corpora_dirs = os.listdir(corpora_path)\n",
    "\n",
    "corpora_files = {} #elements are like corpus:[files]\n",
    "for corpus in corpora_dirs:\n",
    "    corpora_files[corpus] = [os.path.join(corpora_path + corpus, f)\n",
    "             for f in os.listdir(corpora_path + corpus)\n",
    "             if os.path.isfile(os.path.join(corpora_path + corpus, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data to construct Word2Vec and Doc2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Here we are going to collect the data in the following ways:\n",
    "\n",
    "    *A list of all sentences(tokenized by words) for each corpus\n",
    "    *A list of documents(tokenized by words) for the entire corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_sentences = {} #elements are like: corpus:[sentences]\n",
    "corpora_documents = [] #elements are like: documents tokenized by words\n",
    "for corpus in corpora_files:\n",
    "    corpora_sentences[corpus] = []\n",
    "    for file in corpora_files[corpus]:\n",
    "        with open(file, 'r') as text_file:\n",
    "            #all files in my corpora have only one line\n",
    "            #because I removed the line brakes during the compilation\n",
    "            text = text_file.read()\n",
    "            \n",
    "            #removing trash read from the sports blog\n",
    "            if corpus == \"esporte\":\n",
    "                text = re.sub(\"^Pesquisar este blog \", '', text)\n",
    "            \n",
    "            #transforming to lowercase\n",
    "            text = normalizer.to_lowercase(text)\n",
    "            \n",
    "            #tokenizing by word - this is for Doc2Vec\n",
    "            document = normalizer.tokenize_words(text)\n",
    "            corpora_documents.append(document)\n",
    "            \n",
    "            #tokenize text by sentence - this is for Word2Vec\n",
    "            sentences = normalizer.tokenize_sentences(text)\n",
    "            #tokenize each sentence by word and add it to the list\n",
    "            sentences = [normalizer.tokenize_words(sent) for sent in sentences]\n",
    "            corpora_sentences[corpus].extend(sentences)\n",
    "    \n",
    "    #this is also for Doc2Vec\n",
    "    tagged_documents = [TaggedDocument(words=d, tags=[str(i)]) \n",
    "                        for i, d in enumerate(corpora_documents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Word2Vecs for each corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Word2Vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vmodels = {} #elements are like corpus: w2vmodel\n",
    "for corpus in corpora_sentences:\n",
    "    w2vmodels[corpus] = Word2Vec(corpora_sentences[corpus],\n",
    "        size=200, window=5, min_count=3, workers=4)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Doc2Vec for the corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2vmodel = Doc2Vec(tagged_documents, vector_size=20, window=2, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('gol', 0.9986592531204224),\n",
       " ('primeiro', 0.9971026182174683),\n",
       " ('placar', 0.9967527985572815),\n",
       " ('fim', 0.9964091777801514),\n",
       " ('fluminense', 0.9958785772323608),\n",
       " ('segundo', 0.9954502582550049),\n",
       " ('título', 0.9946879148483276),\n",
       " ('contra', 0.9945873618125916),\n",
       " ('início', 0.994193971157074),\n",
       " ('último', 0.9937357902526855)]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodels['esporte'].wv.most_similar('jogo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('eletricidade', 0.9373791813850403),\n",
       " ('corrente', 0.9100709557533264),\n",
       " ('emitida', 0.8989173173904419),\n",
       " ('eletromagnética', 0.8983625173568726),\n",
       " ('partir', 0.8889291286468506),\n",
       " ('colheita', 0.8823054432868958),\n",
       " ('estática', 0.8803973197937012),\n",
       " ('determinado', 0.8752703666687012),\n",
       " ('transformado', 0.875077486038208),\n",
       " ('elétrica', 0.8737519383430481)]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vmodels['ciencia_e_tecnologia'].wv.most_similar('energia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_sci1 = d2vmodel.infer_vector(corpora_documents[0])\n",
    "vector_sci2 = d2vmodel.infer_vector(corpora_documents[1])\n",
    "vector_sport1 = d2vmodel.infer_vector(corpora_documents[-1])\n",
    "vector_sport2 = d2vmodel.infer_vector(corpora_documents[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.16041719913482666\n",
      "0.22453200817108154\n",
      "0.0733189582824707\n",
      "0.22453200817108154\n"
     ]
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "print(spatial.distance.cosine(vector_sci1, vector_sci1))\n",
    "print(spatial.distance.cosine(vector_sci1, vector_sci2))\n",
    "print(spatial.distance.cosine(vector_sci2, vector_sport2))\n",
    "print(spatial.distance.cosine(vector_sport1, vector_sport2))\n",
    "print(spatial.distance.cosine(vector_sci2, vector_sport2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
