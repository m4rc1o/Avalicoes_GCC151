{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marcio/.virtualenvs/AvaliacoesGCC151_env/lib/python3.6/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.25.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import os\n",
    "from utils.lexical import normalizador"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading corpora data to main memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpora_path = '../data/corpora/'\n",
    "corpora_dirs = os.listdir(corpora_path)\n",
    "normalizer = normalizador.Normalizador()\n",
    "\n",
    "corpora = {}\n",
    "for corpus in corpora_dirs:\n",
    "    files = [os.path.join(corpora_path + corpus, f)\n",
    "             for f in os.listdir(corpora_path + corpus)\n",
    "             if os.path.isfile(os.path.join(corpora_path + corpus, f))]\n",
    "    corpora[corpus] = {'raw' : [],'s_tokenized' : [], 'w_tokenized' : [], 's_w_tokenized' : []}\n",
    "    for file in files:\n",
    "        with open(file, 'r', encoding='utf-8') as txt_file:\n",
    "            text = txt_file.readlines()\n",
    "            corpora[corpus]['raw'].extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpora['esporte']['raw'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Removing trash read from the sports blog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "remove_pattern = \"^Pesquisar este blog \"\n",
    "for i in range(len(corpora['esporte']['raw'])):\n",
    "    text = corpora['esporte']['raw'][i]\n",
    "    corpora['esporte']['raw'][i] = re.sub(remove_pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpora['esporte']['raw'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing by sentences, words and both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus in corpora:\n",
    "    for text in corpora[corpus]['raw']:\n",
    "        #tokenizing by sentences\n",
    "        sentences = normalizer.tokenize_sentences(text)\n",
    "        corpora[corpus]['s_tokenized'].append(sentences)\n",
    "        \n",
    "        #tokenizing by sentence and by words in sequence\n",
    "        words_by_sent_list = [normalizer.tokenize_words(sent) for sent in sentences]\n",
    "        corpora[corpus]['s_w_tokenized'].append(words_by_sent_list)\n",
    "        \n",
    "        #transforming in lower case and removing accents and puntuation\n",
    "        text_normalized = normalizer.to_lowercase(text)\n",
    "        text_normalized = normalizer.remove_accents(text_normalized)\n",
    "        text_normalized = normalizer.remove_punctuation(text_normalized)\n",
    "        \n",
    "        #tokenizing by words\n",
    "        words = normalizer.tokenize_words(text_normalized)\n",
    "        corpora[corpus]['w_tokenized'].append(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting word frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "word_frequencies = {} # elements are like corpus: {word : frquence}\n",
    "for corpus in corpora:\n",
    "    word_frequencies[corpus] = {}\n",
    "    for tokens_list in corpora[corpus]['w_tokenized']:\n",
    "        words = normalizer.remove_stopwords(tokens_list)\n",
    "        for w in words:\n",
    "            if w not in word_frequencies[corpus]:\n",
    "                word_frequencies[corpus][w] = 1\n",
    "            else:\n",
    "                word_frequencies[corpus][w] += 1         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(word_frequencies['esporte'])\n",
    "#print(word_frequencies['ciencia_e_tecnologia'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The 20 words frequent most"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ciencia_e_tecnologia:\n",
      "\n",
      "\t\tnao => 1101\n",
      "\t\tser => 968\n",
      "\t\tequipe => 750\n",
      "\t\tsao => 720\n",
      "\t\tpode => 626\n",
      "\t\tenergia => 584\n",
      "\t\tforma => 542\n",
      "\t\tinteligencia => 532\n",
      "\t\tartificial => 527\n",
      "\t\timagem => 488\n",
      "\t\tuniversidade => 482\n",
      "\t\ttambem => 464\n",
      "\t\tmadeira => 463\n",
      "\t\tainda => 450\n",
      "\t\tpodem => 436\n",
      "\t\ttecnologia => 415\n",
      "\t\tcalor => 413\n",
      "\t\tsistema => 412\n",
      "\t\tdados => 410\n",
      "\t\tdisse => 407\n",
      "\n",
      "esporte:\n",
      "\n",
      "\t\tnao => 1421\n",
      "\t\tja => 503\n",
      "\t\ttime => 478\n",
      "\t\tsao => 457\n",
      "\t\tjogo => 453\n",
      "\t\tequipe => 451\n",
      "\t\tcontra => 423\n",
      "\t\tcopa => 390\n",
      "\t\tser => 368\n",
      "\t\tdois => 368\n",
      "\t\tfinal => 365\n",
      "\t\ttambem => 364\n",
      "\t\tbrasileiro => 362\n",
      "\t\tbrasil => 355\n",
      "\t\tpartida => 344\n",
      "\t\tainda => 342\n",
      "\t\t1 => 337\n",
      "\t\tapos => 334\n",
      "\t\ttempo => 326\n",
      "\t\tgol => 326\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import math\n",
    "\n",
    "increasing_freqs = {}\n",
    "for corpus in word_frequencies:\n",
    "    decreasing_freqs = sorted(word_frequencies[corpus].items(), key=operator.itemgetter(1), reverse=True)\n",
    "    increasing_freqs[corpus] = sorted(word_frequencies[corpus].items(), key=operator.itemgetter(1))\n",
    "    print('\\n' + corpus + \":\\n\")\n",
    "    for word, freq in decreasing_freqs[:20]:\n",
    "        print(\"\\t\\t{} => {}\".format(word, freq) )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The 20 least frequent words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ciencia_e_tecnologia:\n",
      "\n",
      "\t\tpropriocepcao => 1\n",
      "\t\tequilibrados => 1\n",
      "\t\tpernas => 1\n",
      "\t\talcancem => 1\n",
      "\t\tverifiquem => 1\n",
      "\t\tmioeletricas => 1\n",
      "\t\trecuperem => 1\n",
      "\t\tvoluntario => 1\n",
      "\t\texplorando => 1\n",
      "\t\tantebraco => 1\n",
      "\t\tfortemente => 1\n",
      "\t\ttornando => 1\n",
      "\t\tadiante => 1\n",
      "\t\texcelentes => 1\n",
      "\t\tsilvestro => 1\n",
      "\t\tmicera => 1\n",
      "\t\trestabelece => 1\n",
      "\t\texternas => 1\n",
      "\t\tinseridos => 1\n",
      "\t\ttraduzir => 1\n",
      "\n",
      "esporte:\n",
      "\n",
      "\t\thornets => 1\n",
      "\t\tfecharam => 1\n",
      "\t\tcontaram => 1\n",
      "\t\tchauncey => 1\n",
      "\t\tbillups => 1\n",
      "\t\tdistribuir => 1\n",
      "\t\tconverter => 1\n",
      "\t\tseattle => 1\n",
      "\t\tsupersonics => 1\n",
      "\t\tdallas => 1\n",
      "\t\tretrospecto => 1\n",
      "\t\tesp => 1\n",
      "\t\toferecidos => 1\n",
      "\t\trecebidos => 1\n",
      "\t\tgaspar => 1\n",
      "\t\tespanhola => 1\n",
      "\t\tconvencelos => 1\n",
      "\t\tliberalo => 1\n",
      "\t\tliberacao => 1\n",
      "\t\tcausada => 1\n"
     ]
    }
   ],
   "source": [
    "for corpus in increasing_freqs:\n",
    "    print('\\n' + corpus + ':\\n')\n",
    "    for word, freq in increasing_freqs[corpus][0:20]:\n",
    "        print(\"\\t\\t{} => {}\".format(word, freq))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The average word size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus ciencia_e_tecnologia:\n",
      "\n",
      "\n",
      "\tAverage word size: 5.363682337516389\n",
      "\n",
      "Corpus esporte:\n",
      "\n",
      "\n",
      "\tAverage word size: 4.733147660752371\n"
     ]
    }
   ],
   "source": [
    "av_word_size_by_corpus = {} #elements are like corpus : av_w_size\n",
    "for corpus in corpora:\n",
    "    total_words, av_word_size = 0, 0\n",
    "    for word_list in corpora[corpus]['w_tokenized']:\n",
    "        for word in word_list:\n",
    "            total_words += 1\n",
    "            av_word_size += len(word)\n",
    "    \n",
    "    av_word_size = av_word_size / total_words\n",
    "    print(\"\\nCorpus {}:\\n\".format(corpus))\n",
    "    print(\"\\n\\tAverage word size: {}\".format(av_word_size))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Average sentence size in number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus ciencia_e_tecnologia\n",
      ":\n",
      "\n",
      "\tAverage number of words by sentece: 29.450447183949723\n",
      "\n",
      "Corpus esporte\n",
      ":\n",
      "\n",
      "\tAverage number of words by sentece: 23.471547536433032\n"
     ]
    }
   ],
   "source": [
    "#Note that here we are counting punctiation\n",
    "for corpus in corpora:\n",
    "    num_of_sentences = 0\n",
    "    num_of_words = 0\n",
    "    for text in corpora[corpus]['s_w_tokenized']:\n",
    "        for sentence in text:\n",
    "            num_of_sentences += 1\n",
    "            num_of_words += len(sentence)\n",
    "    \n",
    "    av_words_by_sent = num_of_words / num_of_sentences\n",
    "    print(\"\\nCorpus {}\\n:\".format(corpus))\n",
    "    print(\"\\n\\tAverage number of words by sentece: {}\".format(av_words_by_sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 20 biggest words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Calculating the size of each word in each corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_sizes = {} #the elements are like corpus:{word: size}\n",
    "for corpus in corpora:\n",
    "    word_sizes[corpus] = {}\n",
    "    for word_list in corpora[corpus]['w_tokenized']:\n",
    "        for word in word_list:\n",
    "            word_sizes[corpus][word] = len(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Printing the 20 biggest words of each corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus ciencia_e_tecnologia:\n",
      "\n",
      "\t* httptecnologiasocialfbborgbr\n",
      "\t* hidretacaomoagemdehidretacao\n",
      "\t* umidificacaodesumidificacao\n",
      "\t* 101103physrevlett121177202\n",
      "\t* 101016jijrefrig201901006\n",
      "\t* 101126sciroboticsaau6914\n",
      "\t* 101016jheliyon2017e00234\n",
      "\t* 101126sciroboticsaav1488\n",
      "\t* 101021acsnanolett8b05051\n",
      "\t* 10108813672630188083041\n",
      "\t* acusticogravitacionais\n",
      "\t* titanioaluminiovanadio\n",
      "\t* 101038s4159801838303x\n",
      "\t* dixoncosmographicacom\n",
      "\t* 101038s41467018062448\n",
      "\t* titanioniobiozirconio\n",
      "\t* 101038s41467018080553\n",
      "\t* benzodioxociclohexeno\n",
      "\t* 101016jjoule201711007\n",
      "\t* 101038nenergy2017144\n",
      "\n",
      "Corpus esporte:\n",
      "\n",
      "\t* corinthianseficiencia\n",
      "\t* setembrocorinthians5\n",
      "\t* diferenciacaotalvez\n",
      "\t* corinthians30081987\n",
      "\t* corinthians12121991\n",
      "\t* corinthians10051998\n",
      "\t* corinthians12052002\n",
      "\t* janeirocorinthians5\n",
      "\t* outubrocorinthians4\n",
      "\t* janeirocorinthians4\n",
      "\t* estatisticasultimo\n",
      "\t* profissionalnumero\n",
      "\t* brasileiro13121990\n",
      "\t* agostocorinthians4\n",
      "\t* tempointernacional\n",
      "\t* coordenadortecnico\n",
      "\t* pinheirosmackenzie\n",
      "\t* experienciajacques\n",
      "\t* supertecnologicajv\n",
      "\t* profissionalizacao\n"
     ]
    }
   ],
   "source": [
    "for corpus in word_sizes:\n",
    "    decreasing_sizes = sorted(word_sizes[corpus].items(), key=operator.itemgetter(1), reverse=True)\n",
    "    print(\"\\nCorpus {}:\\n\".format(corpus))\n",
    "    for word, freq in decreasing_sizes[:20]:\n",
    "        print(\"\\t* {}\".format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The 20 most frequent bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Corpus ciencia_e_tecnologia:\n",
      "\n",
      "\t('inteligencia', 'artificial') => 470\n",
      "\t('pode', 'ser') => 294\n",
      "\t('disse', 'professor') => 202\n",
      "\t('podem', 'ser') => 162\n",
      "\t('celulas', 'solares') => 141\n",
      "\t('temperatura', 'ambiente') => 125\n",
      "\t('et', 'al') => 120\n",
      "\t('comprimento', 'onda') => 99\n",
      "\t('lei', 'termodinamica') => 92\n",
      "\t('ate', 'agora') => 89\n",
      "\t('agua', 'salgada') => 82\n",
      "\t('colegas', 'universidade') => 78\n",
      "\t('aprendizagem', 'profunda') => 75\n",
      "\t('resistente', 'fogo') => 75\n",
      "\t('ponto', 'vista') => 75\n",
      "\t('primeira', 'vista') => 74\n",
      "\t('ampla', 'gama') => 72\n",
      "\t('gerado', 'maquina') => 72\n",
      "\t('espumas', 'metalicas') => 72\n",
      "\t('eletricidade', 'partir') => 72\n",
      "\n",
      "Corpus esporte:\n",
      "\n",
      "\t('sao', 'paulo') => 258\n",
      "\t('campeonato', 'brasileiro') => 99\n",
      "\t('copa', 'mundo') => 94\n",
      "\t('segundo', 'tempo') => 83\n",
      "\t('primeiro', 'tempo') => 82\n",
      "\t('copa', 'brasil') => 82\n",
      "\t('neste', 'domingo') => 71\n",
      "\t('oitavas', 'final') => 70\n",
      "\t('neste', 'sabado') => 64\n",
      "\t('selecao', 'brasileira') => 62\n",
      "\t('formula', '1') => 57\n",
      "\t('serie', 'b') => 53\n",
      "\t('1', '0') => 50\n",
      "\t('ano', 'passado') => 50\n",
      "\t('2', '1') => 49\n",
      "\t('ainda', 'nao') => 49\n",
      "\t('primeira', 'fase') => 47\n",
      "\t('ultima', 'rodada') => 45\n",
      "\t('pode', 'ser') => 43\n",
      "\t('primeira', 'vez') => 41\n"
     ]
    }
   ],
   "source": [
    "for corpus in corpora:\n",
    "    #gets all bigrams in the corpus\n",
    "    bigrams = []\n",
    "    for tokens_list in corpora[corpus]['w_tokenized']:\n",
    "        #removing stop words to get more interesting results\n",
    "        word_list = normalizer.remove_stopwords(tokens_list)\n",
    "        bigrams.extend(list(nltk.bigrams(word_list)))\n",
    "    \n",
    "    #calculates the frequencies of each bigram\n",
    "    bigrams_freq = {} #elements are like str(bigram) : freq\n",
    "    for bigram in bigrams:\n",
    "        if str(bigram) not in bigrams_freq:\n",
    "            bigrams_freq[str(bigram)] = 1\n",
    "        else:\n",
    "            bigrams_freq[str(bigram)] += 1\n",
    "    \n",
    "    #sorts the bigrams by frequence in decreasing order\n",
    "    ordered_bigram_freqs = sorted(bigrams_freq.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    #prints the 20 most frequent bigrams by corpus\n",
    "    print(\"\\nCorpus {}:\\n\".format(corpus))\n",
    "    for str_bigram, freq in ordered_bigram_freqs[:20]:\n",
    "        print(\"\\t{} => {}\".format(str_bigram, freq))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
